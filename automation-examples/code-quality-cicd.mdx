---
title: "Code Quality & CI/CD"
description: "Monitor and improve code quality, automate CI/CD workflows, and handle build failures"
---

## Overview

Code quality and CI/CD automations help maintain high standards and reduce manual intervention when builds fail. These automations can detect issues, attempt fixes, and keep your development workflow smooth.

---

## Example: CI Failure Auto-Fix Monitor

**Use Case**: Automatically detect and attempt to fix failing CI checks on pull requests.

**How it works**:
- Monitors pull requests for failing CI checks
- Analyzes failure logs to understand the issue
- Attempts to fix common issues (linting, formatting, type errors)
- Pushes fixes to the PR branch
- Comments on the PR with what was fixed

**Trigger**: PR check failure webhook

**Prompt Template**:

```text
A CI check has failed on PR #{pr_number}.

1. Fetch the CI logs for the failing check
2. Analyze the error messages to determine the type of failure:
   - Linting errors
   - Type errors
   - Test failures
   - Build errors

3. If it's an auto-fixable issue (linting, formatting):
   - Clone the PR branch
   - Run the appropriate fix command (pnpm run lint --fix, pnpm run pretty)
   - Commit and push the fixes
   - Comment on the PR: "ðŸ¤– Automatically fixed {issue_type}"

4. If it's not auto-fixable:
   - Comment on the PR with a summary of the error
   - Tag the PR author
   - Suggest potential solutions based on the error

5. Update the PR status comment to show Tembo is working on it
```

**MCP Servers Needed**: GitHub, Linear

**Safety Considerations**:
- Only auto-fix low-risk issues (formatting, linting)
- Always commit fixes separately with clear messages
- Never auto-fix test failures or build errors without human review

---

## Tips for Code Quality & CI/CD Automations

### Start with Low-Risk Fixes

Begin with automations that only fix formatting and linting issues. These are safe to auto-fix and won't break functionality.

### Always Notify the Team

When an automation makes changes to code, always leave a comment explaining what was fixed and why. This maintains transparency.

### Set Clear Boundaries

Define explicitly what the automation can and cannot fix. For example, never auto-fix failing tests without human review.

### Track Success Rates

Monitor how often your automation successfully fixes issues versus how often manual intervention is needed. Use this data to improve your prompts.

### Integrate with Issue Tracking

For issues that can't be auto-fixed, automatically create tickets in Linear or Jira with details about the failure.
